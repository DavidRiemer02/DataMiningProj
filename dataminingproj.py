# -*- coding: utf-8 -*-
"""DataMiningProj.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19e-OTAz2MPoDf5yH4tf-txqOnI1v6h8l

# Setup
"""

!pip install pyspark
!pip install -U -q PyDrive
!apt install openjdk-8-jdk-headless -qq
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

import pyspark
from pyspark.sql import *
from pyspark.sql.functions import *
from pyspark import SparkContext, SparkConf

from pyspark.sql import SQLContext
from pyspark import SparkContext

"""Initialize spark context"""

conf = SparkConf().set("spark.ui.port", "4050")

# create the context
sc = pyspark.SparkContext(conf=conf)
spark = SparkSession.builder.getOrCreate()

sqlContext = SQLContext(spark)

"""Read in the data"""

train = spark.read.csv('/content/train.csv', header=True, inferSchema=True)
test = spark.read.csv('/content/test.csv', header=True, inferSchema=True)

"""# Prepare the Data"""

train.printSchema()
train.show(10)

test.printSchema()

from pyspark.sql.functions import col

train.groupBy("Sentiment").count().orderBy(col("count").desc()).show()

from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, HashingTF, IDF
from pyspark.ml.classification import LogisticRegression
# regular expression tokenizer
regexTokenizer = RegexTokenizer(inputCol="SentimentText", outputCol="words", pattern="\\W")

# stop words
stopwordsRemover = StopWordsRemover(inputCol="words", outputCol="filtered").setStopWords(["http","https","amp","rt","t","c","the"])

#converts words into numerical values
hashingTF = HashingTF(inputCol="filtered", outputCol="features")
hashingTF.setNumFeatures(1000)
idf= IDF(inputCol="features", outputCol="rawfeatures", minDocFreq=5)

from pyspark.ml import Pipeline
from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler
label_stringIdx = StringIndexer(inputCol = "Sentiment", outputCol = "label")

pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, label_stringIdx])

# Fit the pipeline to training documents.
pipelineFit = pipeline.fit(train)
dataset = pipelineFit.transform(train)
dataset.show(5)

lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)
lrModel = lr.fit(dataset)

predictions = lrModel.transform(dataset)

predictions.filter(predictions['prediction'] == 0).select("SentimentText","Sentiment","probability","label","prediction")\
.orderBy("probability", ascending=False).show(n = 100, truncate = 30)